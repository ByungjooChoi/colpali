{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: ColQwen과 Elasticsearch `rank_vectors`를 활용한 시각 문서 검색 (RVL-CDIP 데이터셋 적용)\n",
    "\n",
    "이 노트북은 Elastic의 Search Labs 블로그 포스트를 기반으로, **실제 RVL-CDIP 데이터셋**과 최신 SOTA 모델을 사용하여 시각 문서 검색을 시연합니다.\n",
    "\n",
    "**주요 특징:**\n",
    "1.  **실제 데이터셋 사용**: 사전에 샘플링된 **RVL-CDIP 문서 이미지(1600장)**를 사용하여, 실제와 유사한 복잡한 문서 검색 환경을 구성합니다.\n",
    "2.  **모델 업그레이드**: ViDoRe 벤치마크에서 높은 성능을 보이는 **`tsystems/colqwen2.5-3b-multilingual-v1.0`**을 사용합니다.\n",
    "3.  **코드 안정성 및 최적화**: 다양한 하드웨어 환경(GPU, CPU, MPS)에서 안정적으로 작동하고 최적의 성능을 낼 수 있도록 모범 사례(Best Practices)를 적용했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: 환경 설정 및 라이브러리 설치\n",
    "\n",
    "**[KR]** 필요한 라이브러리를 설치합니다.\n",
    "**[EN]** Install the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"git+https://github.com/illuin-tech/colpali.git\"\n",
    "!pip install -q \"transformers>=4.41.0\" accelerate Pillow elasticsearch python-dotenv tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: 라이브러리 임포트 및 환경 변수 설정\n",
    "\n",
    "**[KR]** 필요한 모든 라이브러리를 임포트하고, 이 노트북과 동일한 디렉터리에 있는 `elastic.env` 파일에서 Elastic Cloud 접속 정보를 로드합니다.\n",
    "**[EN]** Import all necessary libraries and load the Elastic Cloud connection information from the `elastic.env` file located in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please create an 'elastic.env' file in the same directory as this notebook and set Elastic_Host and Elastic_API_Key variables.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m ELASTIC_API_KEY = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mElastic_API_Key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ELASTIC_HOST \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ELASTIC_API_KEY:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease create an \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdotenv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m file in the same directory as this notebook and set Elastic_Host and Elastic_API_Key variables.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m INDEX_NAME = \u001b[33m\"\u001b[39m\u001b[33mrvl-cdip-colqwen-demo\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m VECTOR_FIELD_NAME = \u001b[33m\"\u001b[39m\u001b[33mcolqwen_vectors\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Please create an 'elastic.env' file in the same directory as this notebook and set Elastic_Host and Elastic_API_Key variables."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from elasticsearch import Elasticsearch\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "dotenv_path = 'elastic.env'\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "ELASTIC_HOST = os.getenv(\"ELASTIC_HOST\")\n",
    "ELASTIC_API_KEY = os.getenv(\"ELASTIC_API_KEY\")\n",
    "\n",
    "if not ELASTIC_HOST or not ELASTIC_API_KEY:\n",
    "    raise ValueError(f\"Please create an '{dotenv_path}' file in the same directory as this notebook and set Elastic_Host and Elastic_API_Key variables.\")\n",
    "\n",
    "INDEX_NAME = \"rvl-cdip-colqwen-demo\"\n",
    "VECTOR_FIELD_NAME = \"colqwen_vectors\"\n",
    "SAMPLES_DIR = \"samples\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: ColQwen 모델 로드 및 최적화\n",
    "\n",
    "**[KR]**\n",
    "ViDoRe 벤치마크에서 높은 성능을 보이는 SOTA(State-of-the-Art) 모델인 **`tsystems/colqwen2.5-3b-multilingual-v1.0`**을 로드합니다.\n",
    "\n",
    "대용량 모델의 안정적인 로딩과 다양한 하드웨어 환경에서의 최적의 성능을 보장하기 위해 다음의 모범 사례(Best Practices)를 적용했습니다.\n",
    "\n",
    "*   **자동 장치 매핑 (`device_map=\\\"auto\\\"`)**: `transformers` 라이브러리가 내부적으로 `accelerate`를 활용하여, 모델의 각 레이어를 사용 가능한 하드웨어(GPU, CPU)에 최적으로 자동 분산 배치합니다. 이는 대용량 모델을 다루는 가장 안정적이고 표준적인 방식입니다.\n",
    "*   **동적 데이터 타입 최적화**: 코드가 실행되는 환경을 감지하여 최적의 데이터 타입을 동적으로 선택합니다.\n",
    "    *   **GPU 환경**: 지원되는 GPU의 추론 성능을 극대화하고 메모리 사용량을 줄이기 위해 `bfloat16`을 사용합니다.\n",
    "    *   **CPU 및 기타 환경**: 폭넓은 호환성과 안정성을 보장하기 위해 표준 `float32`를 사용합니다.\n",
    "\n",
    "**[EN]**\n",
    "Load the **`tsystems/colqwen2.5-3b-multilingual-v1.0`**, a state-of-the-art (SOTA) model known for its high performance on the ViDoRe benchmark.\n",
    "\n",
    "The following best practices were applied for stable loading and optimal performance across various hardware environments:\n",
    "\n",
    "*   **Automatic Device Mapping (`device_map=\\\"auto\\\"`)**: This allows the `transformers` library to internally leverage `accelerate` for optimally and automatically distributing the model's layers across available hardware (GPU, CPU). It is the most stable and standard method for handling large models.\n",
    "*   **Dynamic Data Type Optimization**: The code detects the execution environment to dynamically select the optimal data type.\n",
    "    *   **On GPU environments**: `bfloat16` is used to maximize inference performance and reduce memory footprint on supported GPUs.\n",
    "    *   **On CPU and other environments**: Standard `float32` is used to ensure broad compatibility and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colpali_engine.models import ColQwen2_5, ColQwen2_5_Processor\n",
    "\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "print(f\"Execution environment detected. Optimal dtype selected: {dtype}\")\n",
    "\n",
    "MODEL_NAME = \"tsystems/colqwen2.5-3b-multilingual-v1.0\"\n",
    "print(f\"\\nLoading model '{MODEL_NAME}'...\")\n",
    "\n",
    "try:\n",
    "    model = ColQwen2_5.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=dtype\n",
    "    ).eval()\n",
    "\n",
    "    processor = ColQwen2_5_Processor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    print(f\"\\n✅ Model '{MODEL_NAME}' loaded successfully!\")\n",
    "    print(f\"Model is using device map: {model.hf_device_map}\")\n",
    "    print(f\"Model is using dtype: {model.dtype}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An error occurred during model loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: RVL-CDIP 샘플 데이터 로드\n",
    "\n",
    "**[KR]** `./samples` 디렉터리에서 사전에 준비된 RVL-CDIP 샘플 이미지들을 로드합니다. 각 이미지의 경로와 PIL Image 객체를 메모리에 저장합니다.\n",
    "**[EN]** Load the pre-prepared RVL-CDIP sample images from the `./samples` directory. Store the path and PIL Image object for each image in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(SAMPLES_DIR):\n",
    "    raise FileNotFoundError(f\"The '{SAMPLES_DIR}' directory was not found. Please run the sampling script first.\")\n",
    "\n",
    "document_images_map = {}\n",
    "image_paths = glob.glob(f'{SAMPLES_DIR}/*/*.png')\n",
    "\n",
    "print(f\"Loading {len(image_paths)} images from '{SAMPLES_DIR}'...\")\n",
    "\n",
    "for path in tqdm(image_paths, desc=\"Loading sample images\"):\n",
    "    category = os.path.basename(os.path.dirname(path))\n",
    "    filename = os.path.basename(path)\n",
    "    doc_id = f\"{category}_{filename}\"\n",
    "    \n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        document_images_map[doc_id] = {\n",
    "            \"image\": img,\n",
    "            \"category\": category,\n",
    "            \"filename\": filename\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load image {path}. Error: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(document_images_map)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: 임베딩 생성 헬퍼 함수 정의\n",
    "\n",
    "**[KR]** 문서 이미지와 텍스트 질의를 입력받아 ColQwen 다중 벡터 임베딩을 생성하는 함수를 정의합니다.\n",
    "**[EN]** Define helper functions to generate ColQwen multi-vector embeddings from document images and text queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_colqwen_document_vectors(image):\n",
    "    inputs = processor.process_images([image]).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.cpu().to(torch.float32).numpy().tolist()[0]\n",
    "\n",
    "def create_colqwen_query_vectors(query_text):\n",
    "    inputs = processor.process_queries([query_text]).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.cpu().to(torch.float32).numpy().tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Elasticsearch 연결 및 인덱스 생성\n",
    "\n",
    "**[KR]** Elasticsearch에 연결하고 `rank_vectors` 필드를 포함하는 인덱스를 생성합니다. `dims`는 128로, `category`와 `original_filename` 필드를 추가합니다.\n",
    "**[EN]** Connect to Elasticsearch and create an index with a `rank_vectors` field. Set `dims` to 128 and add `category` and `original_filename` fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(cloud_id=ELASTIC_HOST.split(':')[0], api_key=ELASTIC_API_KEY) if ':' in ELASTIC_HOST else Elasticsearch(ELASTIC_HOST, api_key=ELASTIC_API_KEY)\n",
    "print(f\"Connected to Elasticsearch version: {es.info()['version']['number']}\")\n",
    "\n",
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "    print(f\"Deleted existing index: {INDEX_NAME}\")\n",
    "\n",
    "mapping = {\n",
    "    \"properties\": {\n",
    "        VECTOR_FIELD_NAME: {\n",
    "            \"type\": \"rank_vectors\",\n",
    "            \"dims\": 128\n",
    "        },\n",
    "        \"category\": { \"type\": \"keyword\" },\n",
    "        \"original_filename\": { \"type\": \"keyword\" }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.create(index=INDEX_NAME, mappings=mapping)\n",
    "print(f\"Created index '{INDEX_NAME}' with a new mapping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: 데이터 인덱싱\n",
    "\n",
    "**[KR]** 로드된 RVL-CDIP 이미지들의 다중 벡터를 추출하여 Elasticsearch에 인덱싱합니다. tqdm을 사용하여 진행 상황을 표시합니다.\n",
    "**[EN]** Extract multi-vectors from the loaded RVL-CDIP images and index them into Elasticsearch. A progress bar is displayed using tqdm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Indexing {len(document_images_map)} documents into '{INDEX_NAME}'...\")\n",
    "\n",
    "for doc_id, data in tqdm(document_images_map.items(), desc=\"Indexing Documents\"):\n",
    "    vectors = create_colqwen_document_vectors(data[\"image\"])\n",
    "    \n",
    "    es_doc = {\n",
    "        VECTOR_FIELD_NAME: vectors,\n",
    "        \"category\": data[\"category\"],\n",
    "        \"original_filename\": data[\"filename\"]\n",
    "    }\n",
    "    \n",
    "    es.index(index=INDEX_NAME, id=doc_id, document=es_doc)\n",
    "\n",
    "es.indices.refresh(index=INDEX_NAME)\n",
    "print(\"\\nIndexing complete.\")\n",
    "print(f\"Total documents in index: {es.count(index=INDEX_NAME)['count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: 검색 및 결과 시각화\n",
    "\n",
    "**[KR]** `script_score`와 `maxSimDotProduct` 함수를 사용하여 늦은 상호작용 검색을 수행하고, 결과를 HTML 테이블로 시각화합니다.\n",
    "**[EN]** Perform a late interaction search using `script_score` and `maxSimDotProduct`, and visualize the results in an HTML table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_display(query):\n",
    "    print(f\"\\nSearching for: '{query}'\")\n",
    "    \n",
    "    query_vectors = create_colqwen_query_vectors(query)\n",
    "    \n",
    "    es_query = {\n",
    "        \"_source\": [\"category\", \"original_filename\"],\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"match_all\": {}},\n",
    "                \"script\": {\n",
    "                    \"source\": f\"maxSimDotProduct(params.query_vector, '{VECTOR_FIELD_NAME}')\",\n",
    "                    \"params\": {\"query_vector\": query_vectors},\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"size\": 5,\n",
    "    }\n",
    "    \n",
    "    results = es.search(index=INDEX_NAME, body=es_query)\n",
    "    \n",
    "    html = \"<table><tr>\"\n",
    "    for hit in results[\"hits\"][\"hits\"]:\n",
    "        doc_id = hit[\"_id\"]\n",
    "        score = hit[\"_score\"]\n",
    "        category = hit[\"_source\"][\"category\"]\n",
    "        filename = hit[\"_source\"][\"original_filename\"]\n",
    "        image = document_images_map[doc_id][\"image\"]\n",
    "        \n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "        \n",
    "        html += f\"\"\"\n",
    "        <td style='text-align: center; vertical-align: top; padding: 10px; border: 1px solid #ddd; max-width: 300px;'>\n",
    "            <img src='data:image/png;base64,{img_str}' style='max-width: 100%; height: auto;'><br>\n",
    "            <b>ID:</b> {doc_id}<br>\n",
    "            <b>Score:</b> {score:.4f}<br>\n",
    "            <b>Category:</b> {category}\n",
    "        </td>\n",
    "        \"\"\"\n",
    "    html += \"</tr></table>\"\n",
    "    \n",
    "    if not results[\"hits\"][\"hits\"]:\n",
    "        print(\"No results found.\")\n",
    "    else:\n",
    "        display(HTML(html))\n",
    "\n",
    "search_and_display(\"a resume with professional experience\")\n",
    "\n",
    "search_and_display(\"과학 논문 초록\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결론\n",
    "\n",
    "**[KR]** 이 노트북을 통해 최신 ColQwen 모델과 Elasticsearch의 `rank_vectors` 필드를 사용하여 실제 RVL-CDIP 문서 이미지를 성공적으로 검색하는 시스템을 구축했습니다. 특히 다국어 모델의 강점을 활용하여 한국어 질의로 영어 문서를 검색하는 것을 시연함으로써, 이 기술의 강력한 활용 가능성을 확인했습니다.\n",
    "\n",
    "**[EN]** In this notebook, we successfully built a system to search real RVL-CDIP document images using the latest ColQwen model and Elasticsearch's `rank_vectors` field. We demonstrated the powerful potential of this technology by leveraging the model's multilingual capabilities to search English documents with a Korean query."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
